---
title: "GPX Analysis 2"
author: "Azimuth1"
date: "August 30, 2018"
output:
  pdf_document:
    fig_caption: yes
    highlight: pygments
    keep_tex: yes
    latex_engine: xelatex
    number_sections: no
    toc: no
  html_document:
    fig_caption: yes
    force_captions: yes
    highlight: pygments
    number_sections: yes
    theme: united
    toc: yes
---

```{r message=FALSE, results = 'hide', warning = FALSE, echo=FALSE}
suppressMessages(library(XML))
suppressMessages(library(plyr))
suppressMessages(library(ggplot2))
#suppressMessages(library(geosphere))

# elevation testing https://stackoverflow.com/questions/21593868/extracting-elevation-from-website-for-lat-lon-points-in-australia-using-r
# FOr google maps API
elevation_key <- 'AIzaSyA_6ejIjLCjU6LHuj4xig1brIHOC5rDzlY'
require(RJSONIO)
```

```{r message=FALSE, results = 'hide', warning = FALSE, echo=FALSE}

temp <- list()
data_dir <- "C:/Users/rober/Desktop/Repositories/gpx_processing/data_backup/"
valid_datasets <- c("14-05.gpx","2016-06-18 10.14.52 Stopwatch.gpx","Current.gpx",
                    "Meg.gpx","Scout.gpx","TeamEchoCurrent.gpx","Track_ 018-02-24 145845.gpx",
                    "Track_14-08 1.gpx","Track_2015-07-05 215817.gpx","Track_2015-07-09 151556.gpx","Track_2015-07-09 183806.gpx","Track_2015-07-10 132200.gpx","Track_2015-07-15 204324.gpx","Track_2015-07-24 164157.gpx","Track_2015-07-29 130706.gpx","Track_2016-04-30 115630.gpx","Track_2016-06-18 131319.gpx","Track_2018-02-10 104731 Task 1.gpx","Track_2018-02-10 110234 Task 2.gpx","Track_2018-02-10 113753 Task 14B.gpx","Track_2018-02-10 150828 Task 29B.gpx","Track_ALPHA05-14 125702.gpx","Track_K9MEG06-18 124555.gpx","Track_QUEBEC5-14 154550.gpx","Track_T18 800 001 TASK2.gpx","Track_T18 800 001.gpx")

hardcode_response <- c("Trail","Sweep","Trail","Canine","Canine","Sweep","Sweep","Sweep","Trail","Trail","Offtrail","Offtrail","Trail","Offtrail","None","Trail","Offtrail","Trail","Trail","Sweep","Trail","Offtrail","Canine","Offtrail","Trail","Sweep")
```

```{r message=FALSE, results = 'hide', warning = FALSE, echo=FALSE}
#### COPIED FUNCTIONS

# Construct dataframe from gpx file
# http://lwlss.net/GarminReports/GarminFunctions.R

# Haversine formula for calculating distances from lat/long
# https://www.r-bloggers.com/great-circle-distance-calculations-in-r/
# Haversine formula is appropriate for calculating distances from lat/long
EarthRad=6371000
haverDist<-function(aLong,aLat,bLong,bLat){
	dLat=2*pi*(bLat-aLat)/360.0; dLon=2*pi*(bLong-aLong)/360.0
	a=(sin(dLat/2))^2+cos(2*pi*aLat/360)*cos(2*pi*bLat/360)*(sin(dLon/2)^2)
	return(EarthRad*2*atan2(sqrt(a),sqrt(1-a)))
}

completeFun <- function(data, desiredCols) {
  completeVec <- complete.cases(data[, desiredCols])
  return(data[completeVec, ])
}

remove_outliers <- function(x) {
  qnt <- quantile(x, probs=c(.25, .75), na.rm = TRUE)
  H <- 1.5 * IQR(x, na.rm = TRUE)
  y <- x
  y[x < (qnt[1] - H)] <- NA
  y[x > (qnt[2] + H)] <- NA
  y
}

remove_by_quantile <- function(x){
  p95 <- quantile(x, 0.95, na.rm = TRUE)
  p05 <- quantile(x, 0.05, na.rm = TRUE)
  return(x[which(x <= p95 & x >= p05)])
}

Mode <- function(x, na.rm = FALSE) {
  if(na.rm){
    x = x[!is.na(x)]
  }

  ux <- unique(x)
  return(ux[which.max(tabulate(match(x, ux)))])
}
```

```{r message=FALSE, results = 'hide', warning = FALSE, echo=FALSE}
# build datasets

qa = NULL

for (i in 1:length(valid_datasets)){
  
  ############
  #XML PARSING
  ############
  name <- valid_datasets[i]
  gpxfile <- paste0(data_dir,valid_datasets[i])
  doc <- xmlParse(gpxfile,useInternalNodes=TRUE)
  top <- xmlRoot(doc)
  
  ############
  #DATA FRAME CONVERSION FOR R
  ############
  
  # GET ELEVATION AND TIME
  trkpts <- top[[2]][[3]]
  data <- toString.XMLNode(trkpts)

  df <- as.data.frame(xmlToDataFrame(data))

  if("extensions" %in% colnames(df)){
    df$extensions <- NULL
  }  
  
  
  # Targeted data extraction
  if(toString.XMLNode(top[[2]][[3]])=="NULL") {
    if(toString.XMLNode(top[[2]][[2]])=="NULL") {
      attribs=xmlSApply(top[[2]][[1]],xmlAttrs)
    }else{
      attribs=xmlSApply(top[[2]][[2]],xmlAttrs)
    }
  }else{
    attribs=xmlSApply(top[[2]][[3]],xmlAttrs)
  }
  
  df$lon=as.numeric(attribs[2,])
  df$lat=as.numeric(attribs[1,])
  
  # Tidy data frame  
  colnames(df)=c("Elevation","DateTime","Longitude","Latitude")
  df$Elevation=as.numeric(df$Elevation)
  df$DateTime=as.character(df$DateTime)
  
  # Convert timestamp to number of seconds since start of run
  # Calculate Seconds
  T0 <- strptime(df$DateTime[1],"%Y-%m-%dT%H:%M:%S")
  df$Seconds <- as.numeric(strptime(df$DateTime,"%Y-%m-%dT%H:%M:%S") - T0)

  # Initialise columns
  df$dNorth=0; df$dEast=0; df$dUp=0;
  df$North=0; df$East=0; df$dDist=0; 
  df$dDist2D=0; df$Dist2D=0

  # Calculate northings and eastings
  df$East <- haverDist(df[1,"Longitude"],df[1,"Latitude"],df$Longitude,df[1,"Latitude"])*sign(df$Longitude-df[1,"Longitude"])
  df$North <- haverDist(df[1,"Longitude"],df[1,"Latitude"],df[1,"Longitude"],df$Latitude)*sign(df$Latitude-df[1,"Latitude"])

  
  # Calculate changes in position for each dt
  for (x in 2:(length(df$DateTime)-1)) {
  	sEast=sign(df[x,"Longitude"]-df[1,"Longitude"])
  	sNorth=sign(df[x,"Latitude"]-df[1,"Latitude"])
  	df$dEast[x]=sEast*haverDist(df[x-1,"Longitude"],df[1,"Latitude"],df[x,"Longitude"],df[1,"Latitude"])
  	df$dNorth[x]=sNorth*haverDist(df[1,"Longitude"],df[x-1,"Latitude"],df[1,"Longitude"],df[x,"Latitude"])
  	df$dUp[x]=df$Elevation[x]-df$Elevation[x-1]
  	# 2D distance (ignoring hills)
  	df$dDist2D[x]=haverDist(df[x-1,"Longitude"],df[x-1,"Latitude"],df[x,"Longitude"],df[x,"Latitude"])
  }

  
  df$dDist=sqrt(df$dNorth^2+df$dEast^2+df$dUp^2)
  df$Dist=cumsum(df$dDist)
  df$Dist2D=cumsum(df$dDist2D)

  # Fit a spline function to the GPS coordinates & elevation
  east=splinefun(df$Seconds,df$East)
  north=splinefun(df$Seconds,df$North)
  up=splinefun(df$Seconds,df$Elevation)
  dist=splinefun(df$Seconds,df$Dist)
  
  df$Speed=rep(0,length(df$Seconds))
  df$Gradient=rep(0,length(df$Seconds))
  df$Slope <- ifelse(df$dDist2D != 0, df$dUp/df$dDist2D, 0)
  
  # Do finite centred differencing to give smoothest rate/gradient estimates
  for(x in 2:(length(df$Seconds)-1)){
  	Dt=df[x+1,"Seconds"]-df[x-1,"Seconds"]
  	Dd=df[x+1,"Dist"]-df[x-1,"Dist"]
  	df[x,"Speed"]=Dd/Dt # m/s
  	df[x,"Gradient"]=(df[x+1,"Elevation"]-df[x-1,"Elevation"])/Dd # m/m
  }
  
  df[1,"Speed"]=df[2,"Speed"]
  df[length(df$Seconds),"Speed"]=df[length(df$Seconds)-1,"Speed"]
  df[1,"Gradient"]=df[2,"Gradient"]
  df[length(df$Seconds),"Gradient"]=df[length(df$Seconds)-1,"Gradient"]
  
  # Smooth speed as it is unrealistically noisy
  df$Speed=smooth(df$Speed)
  
  # Fit a spline function to rate
  speed=splinefun(df$Seconds,df$Speed)
  pace<-function(t) sapply(1/speed(t),max,0)
  ppace<-function(t) 1000*pace(t)/60
  
  # Update dataframe with speed and pace
  df$Speed=speed(df$Seconds)
  df$Pace=pace(df$Seconds)
  
  df$dt_ts <- strptime(df$DateTime, "%Y-%m-%dT%H:%M:%SZ")
  df$name <- name
  df$type <- hardcode_response[i]
  
  # Code mod -- QA dataframe and a slope function
  qa = rbind(qa, data.frame(i, name))
  
  temp[[name]] <- df
}

df_all <- do.call("rbind", temp)
df_all$ID <- seq.int(nrow(df_all))
```



```{r message=FALSE, results = 'hide', warning = FALSE, echo=FALSE}
df_all$Speed_mph <- df_all$Speed*2.23694

df_all_clean<-df_all[ which(df_all$Speed_mph < 5.5), ]
df_all_clean<-df_all_clean[df_all_clean$type != "None", ]
#df_all_clean<-df_all_clean[df_all_clean$Gradient != 1, ]
#df_all_clean<-df_all_clean[df_all_clean$Speed != -1, ]
df_all_clean<- df_all_clean[is.finite(df_all_clean$Pace), ]
df_all_clean<-completeFun(df_all_clean, "Gradient")

df_all_clean$gradient_bin <- cut(df_all_clean$Gradient, breaks = seq(-1, 1, by = .2),labels= c("-1:-0.8","-0.8:-0.6","-0.6:-0.4","-0.4:-0.2","-0.2:0","0:0.2","0.2:0.4","0.4:0.6","0.6:0.8","0.8:1"))

# Fix for dplyr
df_all_clean_mod <- df_all_clean
df_all_clean_mod$dt_ts <- as.POSIXct(df_all_clean_mod$dt_ts)

  filename_lookup <- data.frame("filename"=unique(df_all_clean_mod$name), "lookup"=1:length(unique(df_all_clean_mod$name)))
  df_all_clean_mod$file_lu <- filename_lookup$lookup[match(df_all_clean_mod$name, filename_lookup$filename)]

```

```{r message=FALSE, results = 'hide', warning = FALSE, echo=FALSE}
suppressMessages(require(dplyr))

df_all_clean_mod<-df_all_clean_mod %>%
  group_by(name) %>% 
  mutate(
    end_time = c(dt_ts[-1]-0.1, NA)
  )

df_all_clean_mod$collection_gap<-as.numeric(abs(difftime(df_all_clean_mod$dt_ts, df_all_clean_mod$end_time, units="secs")))

df_all_clean_mod$capture_common <- ifelse(df_all_clean_mod$collection_gap == Mode(df_all_clean_mod$collection_gap), TRUE,FALSE)
```

### a. What did you use as your interval for the count analysis?  Was it simply the interval they had set, or was it something like every 5 minutes?  If it was what they simply had set, what was the most common interval seen?

- No modifications were made to the GPS capture interval. Virtually all data was captured sporadically. No devices were recognized to have any consistent pattern. The mean collection interval for all data is `r mean(df_all_clean_mod$collection_gap, na.rm=TRUE)`  seconds. The SD is `r sd(df_all_clean_mod$collection_gap, na.rm=TRUE)`. The following shows an arbitrary >25%, <75% interquartile range to demonstrate the collection ranges. The most common interval is `r Mode(df_all_clean_mod$collection_gap)`.

```{r, warning = FALSE, echo=FALSE, fig.height=6, fig.align='center'}

par(mfrow=c(2,2))
with(df_all_clean_mod,boxplot(collection_gap~type, main="Collection Intervals By Type",las=3, ylab="Seconds",cex.main=0.8))

with(df_all_clean_mod,boxplot(collection_gap~file_lu, main="Collection Intervals By GPX File",las=2, cex.axis=0.5,cex.main=0.8))
axis(1, labels = FALSE)

with(df_all_clean_mod,boxplot(remove_outliers(collection_gap)~type, main="Collection Intervals By Type (Outliers Removed)",las=3, ylab="Seconds",cex.main=0.8))

with(df_all_clean_mod,boxplot(remove_outliers(collection_gap)~file_lu, main="Collection Intervals By GPX File (Outliers Removed)",las=2, xlab="", cex.axis=0.5,cex.main=0.8))
#labels<- unique(df_all_clean_mod$name)
#a#xis(1, labels = FALSE)
#text(x =  seq_along(labels), y = par("usr")[3] - 1, srt = 45, adj = 1,labels = labels, xpd = TRUE)
```

\newpage
```{r, results='asis',  warning = FALSE, echo=FALSE}
knitr::kable(filename_lookup[2:1], caption="GPX File Index Lookup")
```


```{r, warning = FALSE, echo=FALSE, fig.height=4}
#ggplot(df_all_clean_mod, aes(remove_outliers(collection_gap), color=type)) +
#  geom_histogram(position="identity", binwidth=3, aes(y=..density.., fill=type),  alpha=0.5) +
#  geom_density()

ggplot(df_all_clean_mod, aes(collection_gap, color=name),) +
  #geom_histogram(position="identity", binwidth=3, aes(y=..density.., fill=name),  alpha=0.5) +
  geom_density()+ theme(legend.position="none") +
       xlim(c(0,100))     +   labs(title="GPS Collection Density per GPX file") +
       labs (y="Density") +
       labs(x="Seconds") 
```


b. I of course see the count, that came from how many actual tasks?

- The number of GPX points collected per category:

```{r, results='asis',  warning = FALSE, echo=FALSE}
knitr::kable(df_all_clean_mod[,-which(names(df_all_clean_mod) == "dt_ts")] %>% 
  group_by(type) %>%
  summarise(number = n()))
```

- The number of GPX points collected per file:

```{r, results='asis',  warning = FALSE, echo=FALSE}
knitr::kable(df_all_clean_mod[,-which(names(df_all_clean_mod) == "dt_ts")] %>% 
  group_by(type,name) %>%
  summarise(number = n()))
```

- The number of files per type:

```{r, results='asis',  warning = FALSE, echo=FALSE}
knitr::kable(df_all_clean_mod[,-which(names(df_all_clean_mod) == "dt_ts")] %>% 
  group_by(type,name) %>%
  summarise(number = n())  %>%   
  group_by(type) %>%
  summarise(number = n()))
```

- The most common increments:

```{r, results='asis',  warning = FALSE, echo=FALSE}
sort(table(df_all_clean_mod$collection_gap), decreasing=TRUE)[1:10]
```

c. For Idaho, did you download some actual search tasks from other searches, during the SAREX I didn't think anyone actually went out into the field.

*Jason*

d. Area sweep,  Did you do any NLCD analysis, to see what the speeds looked like in open terrain vs canopy?  Looking at the chart of previous results, I remain dubious of the open terrain results.

*Jason*

e. Looking at the slope bins,  I see 0.2:0.4 etc.  I take it that 1 equals a 90 degree slope and 0.5 would be a 45% degree slope? Later on I see you call it a gradient.

The gradient represents a smoothed +/- 1 representation of slope. It is not a 1:1 relationship as shown on the following graph. A large degree of points collected are well outside of normal hiking steepness. Each gradient exists as a calculation inside of each particular GPX file, and not as a normalization of data as a whole. This is due to the idiosyncratic behavior of each user and device. A few examples

```{r, warning = FALSE, echo=FALSE}
par(mfrow=c(2,2))
#gradient_subset <- subset(df_all, Gradient > -1 & Gradient < 1 & name == '14-05.gpx')

ggplot(subset(df_all, name == '14-05.gpx'), aes(Gradient,Slope, col=Gradient)) + 
  geom_point() + 
  stat_smooth() 

ggplot(subset(df_all, name == 'Track_2018-02-10 113753 Task 14B.gpx'), aes(Gradient,Slope, col=Gradient)) + 
  geom_point() + 
  stat_smooth() 
 
```

f. Cleary the team move much faster on flat terrain.  For the sweep task, it seems to pick up at a gradient of 0.6.   For the calculation of PSR, I'm open to your suggestions on the best way to integrate this data.  Also if for the sweep task we also look at no canopy vs canopy.

```{r}

```

g. For the off trail it was interesting to see speeds of 2 mph.  On previous sweep width experiments the speeds have been 1.75km/hr or about 1.0 mph.  This would suggest searchers are nearly twice as fast on actual searches.  I might need to conduct an experiment to test different search speeds for a possible correction factor.

```{r}

```

h. We have some pretty big standard deviations, did you run any statistical analysis to see if the on trail and off trail where statically significant differences?

```{r}

```
